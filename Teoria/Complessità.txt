L'esecuzione di un programma implica un costo economico, dovuto all'utilizzo delle risorse e di tempo di elaborazione.
In informatica il calcolo asintotico è utilizzato per analizzare la complessità di un algoritmo.
In genere si parla di

Complessità spaziale: per intendere l'utilizzo delle risorse(memoria) da parte di un programma.
Complessità temporale: per intendere il tempo di esecuzione di un programma.

La complessità temporale cresce al crescere della dimensione n dei dati in input.

Notazione asintotica O grande(caso peggiore)
-----------------------------
Data un'istanza di dimensione n, nel caso peggiore l'algoritmo ha una complessità temporale O(f(n)) se T(n)=O(f(n)).
Dove n è il numero delle righe eseguite mentre f(n) è un limite asintotico superiore del tempo di esecuzione dell'algoritmo nell'ipotesi peggiore.

Notazione asintotica Omega grande(caso migliore)

Notazione asintotica Teta grande(caso medio)

Per un dato problema P consideriamo un algoritmo A che lo risolve:

- Se A prende tempo t(n) diremo che O(t(n)) è un limite asintotico superiore.

- Se riusciamo a provare che nessun algoritmo può far meglio di t(n) diremo che Ω(t(n)) è un limite asintotico inferiore.

- A è ottimo se i due limiti coincidono e in tal caso la complessità computazionale del problema è Θ(t(n)).

Un algoritmo A che risolve un problema P è ottimale se:
1. P ha complessità Ω(f(n))   //nel caso migliore ho una certa complessità
2. A ha complessità O(f(n))   //nel caso peggiore ho la stessa complessità

Esempio: il problema dell’ordinamento ha complessità Ω(nlogn). Quindi un algoritmo di ordinamento con complessità O(nlogn) è ottimale.

  -Nel selection sort non si può fare meglio di 0(n alla 2)
  -Si può migliorare l'insertion sort con la ricerca binaria per collocare l'elemento corrente

O(1) :
----------
complessità di una funzione o blocco di istruzioni ciascuna di costo O(1), che non contengono cicli,
ricorsione o chiamate ad altre funzioni non costanti.

O(n) : 
---------
complessità di un ciclo quando le sue variabili (es. contatore) sono incrementate/decrementate di una quantità costante.

// c è una costante positiva

for (int i = 0; i <= n; i += c) 
{
  //espressioni con costo O(1)
}


O(n alla c) : 
---------
la complessità di cicli annidati è uguale al numero di volte in cui le istruzioni del ciclo interno
vengono eseguite.

// c è una costante positiva

for(int i = 1; i <=n; i += c) 
{
   for (int j = 1; j <=n; j += c) 
   {
     //espressioni con costo O(1)
   }
}

O(Logn) : 
------
complessità di un ciclo quando le sue variabili sono incrementate/decrementate 
moltiplicandole/dividendole per una costante.

// c è una costante positiva

for (int i = 1; i <= n; i *= c) 
{
  //espressioni con costo O(1)
}

for (int i = 1; i <= n; i /= c) 
{
  //espressioni con costo O(1)
}

O(LogLogn) : 
-----------
complessità di un ciclo quando le sue variabili sono incrementate/decrementate esponenzialmente.

// c è una costante positiva > 1

for(int i = 2; i <=n; i = pow(i,c)) 
{
  //espressioni con costo O(1)
}

RELAZIONE DI RICORRENZA
-----------------------
Le relazioni di ricorrenza descrivono in modo preciso le prestazioni degli algoritmi ricorsivi in esame.

Ricorrenze fondamentali: 

C = (n^2)/2
-----------
Questa formula si usa per programmi che ciclano sull’input ‘’eliminando’’ un elemento per volta.

C = log n
-----------
Questa formula si usa tipicamente con un programma ricorsivo che dimezza l’input ad ogni passo

C = 2n
----------
Questa formula si usa tipicamente con un programma ricorsivo che dimezza l’input ed esamina,
eventualmente, ogni elemento di esso

oppure 

Questa formula si usa tipicamente con un programma ricorsivo che dimezza l’input ed esegue una quantità di lavoro addizionale costante.

C = n log n
------------
Questa formula si usa tipicamente con un programma ricorsivo che esegue una scansione lineare dell’input prima, durante, oppure dopo aver suddiviso l’input in due parti.

BST
------------
le operazioni su insiemi dinamici Insert e Delete possono essere eseguite su un albero binario di ricerca di altezza h in in tempo O(h).
Tutte le operazioni su BST hanno un costo O(h) dove h è l’altezza dell’albero binario di ricerca.
È fondamentale mantenere l’albero bilanciato, in questo caso infatti l’altezza è logaritmica del numero di nodi

Complessità dei metodi di visita: Log n

-Albero binario completo:
    da ogni nodo partono sempre due rami e le foglie sono tutte allo stesso livello.

-Albero binario perfettamente bilanciato:
    per ogni nodo il numero dei nodi del sottoalbero sinistro differisce al più di 1 dal numero dei nodi del sottoalbero destro.

FUNZIONI INLINE:
-----------------
Le funzioni inline vengono eseguite più velocemente a causa di una macro interna.

Quando si progetta una classe è bene separare l'implementazione dei metodi dalla loro definizione.
Scorporare il codice in questo modo, comporta un guadagno in termini di spazio (occupazione della
memoria) in quanto il cambio di contesto prevede che venga salvato solo l'indirizzo di rientro
ma comporta un costo pesante in termini di prestazioni(velocità di risposta).

Dichiarare una funzione inline significa invece non separare l'implementazione del metodo dalla
propria dichiarazione. In questo modo ogni volta che viene invocato il metodo il compilatore
è come se sostituisse il blocco di codice della funzione al posto della sua chiamata.

Non essendoci un cambio di contesto la dichiarazione inline di una funzione, pur sacrificando spazio,
guadagna in termini di prestazioni(velocità).
